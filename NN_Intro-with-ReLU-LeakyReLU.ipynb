{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9345fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74716e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Pecan.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846481e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x14e19c154d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['Salinity level'], df['Pecan Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09911309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Row ID column\n",
    "data = df.drop(columns=['Row ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4b1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input (features) and output (target)\n",
    "X = data.drop(columns=['Pecan Yield']).values\n",
    "y = data['Pecan Yield'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f591d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch scikit-learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a46a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#  Specifies the proportion of the dataset that \n",
    "# should be allocated to the test set. \n",
    "# Here, 0.2 means 20% of the data will be used\n",
    "# for testing, and the remaining 80% will be \n",
    "# used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49de71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# StandardScaler: This is a class from \n",
    "# Scikit-learn used to standardize \n",
    "# features by removing the mean and \n",
    "# scaling to unit variance.\n",
    "# Standardization: This process involves\n",
    "# rescaling the features so that they have \n",
    "# a mean of 0 and a standard deviation of 1.\n",
    "# This is important for many machine learning\n",
    "# algorithms that perform better when \n",
    "# features are on a similar scale.\n",
    "\n",
    "# This process is crucial for ensuring \n",
    "# that the model performs consistently,\n",
    "# as many machine learning algorithms \n",
    "# assume or perform better when the \n",
    "# input features are on a similar scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0743cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(\n",
    "    X_train, dtype=torch.float32)\n",
    "\n",
    "# torch.tensor(X_train): Converts \n",
    "# the X_train data (which is likely \n",
    "# a NumPy array or a Pandas \n",
    "# DataFrame) into a PyTorch tensor.\n",
    "# dtype=torch.float32: Specifies that \n",
    "# the data type of the tensor should \n",
    "# be float32, which is a common \n",
    "# choice for numerical data in \n",
    "# deep learning models.\n",
    "# Result: X_train_tensor is now a \n",
    "# PyTorch tensor containing the \n",
    "# standardized training input data, \n",
    "# ready for use in a neural network.\n",
    "\n",
    "y_train_tensor = torch.tensor(\n",
    "    y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# torch.tensor(y_train): Converts the \n",
    "# y_train data (the target variable \n",
    "# for training) into a PyTorch tensor.\n",
    "# .view(-1, 1): Reshapes the tensor \n",
    "# to have a shape of (-1, 1). \n",
    "# Here, -1 is a placeholder that tells \n",
    "# PyTorch to infer the correct size for \n",
    "# this dimension based on the other \n",
    "# dimensions. This ensures \n",
    "# that y_train_tensor has two dimensions, \n",
    "# with the second dimension being 1 \n",
    "# (i.e., a column vector).\n",
    "# Result: y_train_tensor is now a 2D \n",
    "# tensor with shape (n_samples, 1), \n",
    "# where n_samples is the number of \n",
    "# training samples.\n",
    "\n",
    "X_test_tensor = torch.tensor(\n",
    "    X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(\n",
    "    y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df318eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(\n",
    "    X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fac1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24214ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "#  defines a new class NeuralNetwork \n",
    "# that inherits from nn.Module. \n",
    "# By inheriting from nn.Module, \n",
    "# the NeuralNetwork class gains access \n",
    "# to all the functionalities provided \n",
    "# by PyTorch for building and managing neural networks.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self): # Constructor\n",
    "        # Start by calling the constructor of \n",
    "        # the parent class (nn.Module), ensuring \n",
    "        # that the class is properly initialized.\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # nn.Linear creates a fully connected (linear) layer.\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    # The forward method defines the forward pass of \n",
    "    # the network. It specifies how the input tensor\n",
    "    # x should flow through the layers of the network.\n",
    "    def forward(self, x):        \n",
    "        x = self.fc1(x)\n",
    "        x=F.leaky_relu(x, negative_slope=0.01)  # x=torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x=F.leaky_relu(x, negative_slope=0.01) # x=torch.relu(x)\n",
    "        x = self.fc3(x)  # Output layer    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26d9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.L1Loss() # for mean absoluste loss\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=0.01)\n",
    "\n",
    "# optim.Adam: This creates an optimizer using the \n",
    "# Adam algorithm, which is a popular optimization\n",
    "# algorithm that combines the advantages of two \n",
    "# other algorithms: AdaGrad and RMSProp. It adjusts\n",
    "# the learning rate for each parameter individually\n",
    "# based on estimates of lower-order moments.\n",
    "# model.parameters(): This passes the model's\n",
    "# parameters (weights and biases) to the optimizer.\n",
    "# The optimizer will update these parameters during\n",
    "# training based on the computed gradients.\n",
    "# lr=0.01: This sets the learning rate, which \n",
    "# controls how much to adjust the model parameters\n",
    "# with respect to the gradient. A learning rate of\n",
    "# 0.01 is relatively standard, but it may need \n",
    "# tuning depending on the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735c4f5-d7b3-42cb-891e-aa7f698ad04c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0f67c-6678-4cdc-a700-0747c7b7f91e",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b54385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Average Loss: 438.1989\n",
      "Epoch 2/200, Average Loss: 435.7981\n",
      "Epoch 3/200, Average Loss: 435.2882\n",
      "Epoch 4/200, Average Loss: 430.4089\n",
      "Epoch 5/200, Average Loss: 424.3401\n",
      "Epoch 6/200, Average Loss: 417.2837\n",
      "Epoch 7/200, Average Loss: 409.5441\n",
      "Epoch 8/200, Average Loss: 393.6500\n",
      "Epoch 9/200, Average Loss: 374.9861\n",
      "Epoch 10/200, Average Loss: 356.6382\n",
      "Epoch 11/200, Average Loss: 332.1789\n",
      "Epoch 12/200, Average Loss: 294.9758\n",
      "Epoch 13/200, Average Loss: 253.1527\n",
      "Epoch 14/200, Average Loss: 220.9356\n",
      "Epoch 15/200, Average Loss: 190.2700\n",
      "Epoch 16/200, Average Loss: 158.5062\n",
      "Epoch 17/200, Average Loss: 121.5469\n",
      "Epoch 18/200, Average Loss: 103.7166\n",
      "Epoch 19/200, Average Loss: 105.3115\n",
      "Epoch 20/200, Average Loss: 102.7310\n",
      "Epoch 21/200, Average Loss: 94.9879\n",
      "Epoch 22/200, Average Loss: 86.6960\n",
      "Epoch 23/200, Average Loss: 75.0179\n",
      "Epoch 24/200, Average Loss: 65.8227\n",
      "Epoch 25/200, Average Loss: 62.1731\n",
      "Epoch 26/200, Average Loss: 56.2441\n",
      "Epoch 27/200, Average Loss: 45.3044\n",
      "Epoch 28/200, Average Loss: 39.5031\n",
      "Epoch 29/200, Average Loss: 35.1242\n",
      "Epoch 30/200, Average Loss: 28.6157\n",
      "Epoch 31/200, Average Loss: 25.0390\n",
      "Epoch 32/200, Average Loss: 24.8155\n",
      "Epoch 33/200, Average Loss: 19.5413\n",
      "Epoch 34/200, Average Loss: 16.3812\n",
      "Epoch 35/200, Average Loss: 13.1654\n",
      "Epoch 36/200, Average Loss: 11.6682\n",
      "Epoch 37/200, Average Loss: 12.2881\n",
      "Epoch 38/200, Average Loss: 10.5256\n",
      "Epoch 39/200, Average Loss: 9.8912\n",
      "Epoch 40/200, Average Loss: 9.2172\n",
      "Epoch 41/200, Average Loss: 8.8558\n",
      "Epoch 42/200, Average Loss: 8.4757\n",
      "Epoch 43/200, Average Loss: 8.5355\n",
      "Epoch 44/200, Average Loss: 7.4259\n",
      "Epoch 45/200, Average Loss: 7.5789\n",
      "Epoch 46/200, Average Loss: 7.5130\n",
      "Epoch 47/200, Average Loss: 6.8109\n",
      "Epoch 48/200, Average Loss: 6.1626\n",
      "Epoch 49/200, Average Loss: 6.4007\n",
      "Epoch 50/200, Average Loss: 6.1451\n",
      "Epoch 51/200, Average Loss: 5.7359\n",
      "Epoch 52/200, Average Loss: 5.8762\n",
      "Epoch 53/200, Average Loss: 5.8012\n",
      "Epoch 54/200, Average Loss: 6.4497\n",
      "Epoch 55/200, Average Loss: 6.9486\n",
      "Epoch 56/200, Average Loss: 6.1529\n",
      "Epoch 57/200, Average Loss: 6.0773\n",
      "Epoch 58/200, Average Loss: 7.0285\n",
      "Epoch 59/200, Average Loss: 5.7908\n",
      "Epoch 60/200, Average Loss: 5.6591\n",
      "Epoch 61/200, Average Loss: 6.4443\n",
      "Epoch 62/200, Average Loss: 5.8559\n",
      "Epoch 63/200, Average Loss: 7.3700\n",
      "Epoch 64/200, Average Loss: 7.0307\n",
      "Epoch 65/200, Average Loss: 6.0943\n",
      "Epoch 66/200, Average Loss: 6.9336\n",
      "Epoch 67/200, Average Loss: 5.4392\n",
      "Epoch 68/200, Average Loss: 5.4753\n",
      "Epoch 69/200, Average Loss: 5.2120\n",
      "Epoch 70/200, Average Loss: 5.8731\n",
      "Epoch 71/200, Average Loss: 6.4532\n",
      "Epoch 72/200, Average Loss: 4.9297\n",
      "Epoch 73/200, Average Loss: 6.3219\n",
      "Epoch 74/200, Average Loss: 5.2565\n",
      "Epoch 75/200, Average Loss: 5.0314\n",
      "Epoch 76/200, Average Loss: 5.3890\n",
      "Epoch 77/200, Average Loss: 4.7703\n",
      "Epoch 78/200, Average Loss: 5.6835\n",
      "Epoch 79/200, Average Loss: 5.9643\n",
      "Epoch 80/200, Average Loss: 5.2864\n",
      "Epoch 81/200, Average Loss: 5.6962\n",
      "Epoch 82/200, Average Loss: 5.7601\n",
      "Epoch 83/200, Average Loss: 4.9889\n",
      "Epoch 84/200, Average Loss: 4.5656\n",
      "Epoch 85/200, Average Loss: 5.2017\n",
      "Epoch 86/200, Average Loss: 5.2376\n",
      "Epoch 87/200, Average Loss: 5.4973\n",
      "Epoch 88/200, Average Loss: 7.1659\n",
      "Epoch 89/200, Average Loss: 7.3024\n",
      "Epoch 90/200, Average Loss: 8.8980\n",
      "Epoch 91/200, Average Loss: 8.1390\n",
      "Epoch 92/200, Average Loss: 7.8450\n",
      "Epoch 93/200, Average Loss: 6.6584\n",
      "Epoch 94/200, Average Loss: 6.7884\n",
      "Epoch 95/200, Average Loss: 5.7966\n",
      "Epoch 96/200, Average Loss: 6.8404\n",
      "Epoch 97/200, Average Loss: 5.4960\n",
      "Epoch 98/200, Average Loss: 4.5408\n",
      "Epoch 99/200, Average Loss: 6.5662\n",
      "Epoch 100/200, Average Loss: 6.0086\n",
      "Epoch 101/200, Average Loss: 6.6743\n",
      "Epoch 102/200, Average Loss: 4.8428\n",
      "Epoch 103/200, Average Loss: 5.0321\n",
      "Epoch 104/200, Average Loss: 7.9679\n",
      "Epoch 105/200, Average Loss: 5.4202\n",
      "Epoch 106/200, Average Loss: 5.0795\n",
      "Epoch 107/200, Average Loss: 6.0320\n",
      "Epoch 108/200, Average Loss: 5.9519\n",
      "Epoch 109/200, Average Loss: 5.5355\n",
      "Epoch 110/200, Average Loss: 5.6607\n",
      "Epoch 111/200, Average Loss: 5.0839\n",
      "Epoch 112/200, Average Loss: 4.5335\n",
      "Epoch 113/200, Average Loss: 4.4256\n",
      "Epoch 114/200, Average Loss: 4.7490\n",
      "Epoch 115/200, Average Loss: 4.4586\n",
      "Epoch 116/200, Average Loss: 4.3668\n",
      "Epoch 117/200, Average Loss: 4.7348\n",
      "Epoch 118/200, Average Loss: 4.5074\n",
      "Epoch 119/200, Average Loss: 4.5335\n",
      "Epoch 120/200, Average Loss: 6.8275\n",
      "Epoch 121/200, Average Loss: 5.5836\n",
      "Epoch 122/200, Average Loss: 5.4630\n",
      "Epoch 123/200, Average Loss: 6.5712\n",
      "Epoch 124/200, Average Loss: 5.4772\n",
      "Epoch 125/200, Average Loss: 6.4546\n",
      "Epoch 126/200, Average Loss: 5.8421\n",
      "Epoch 127/200, Average Loss: 7.1690\n",
      "Epoch 128/200, Average Loss: 6.1111\n",
      "Epoch 129/200, Average Loss: 4.6928\n",
      "Epoch 130/200, Average Loss: 6.2141\n",
      "Epoch 131/200, Average Loss: 4.3973\n",
      "Epoch 132/200, Average Loss: 7.4237\n",
      "Epoch 133/200, Average Loss: 4.9348\n",
      "Epoch 134/200, Average Loss: 5.1978\n",
      "Epoch 135/200, Average Loss: 4.3115\n",
      "Epoch 136/200, Average Loss: 5.1341\n",
      "Epoch 137/200, Average Loss: 4.5935\n",
      "Epoch 138/200, Average Loss: 4.1129\n",
      "Epoch 139/200, Average Loss: 4.1977\n",
      "Epoch 140/200, Average Loss: 4.5230\n",
      "Epoch 141/200, Average Loss: 5.3324\n",
      "Epoch 142/200, Average Loss: 5.2031\n",
      "Epoch 143/200, Average Loss: 6.4723\n",
      "Epoch 144/200, Average Loss: 4.5767\n",
      "Epoch 145/200, Average Loss: 4.9810\n",
      "Epoch 146/200, Average Loss: 4.3897\n",
      "Epoch 147/200, Average Loss: 4.3785\n",
      "Epoch 148/200, Average Loss: 4.9594\n",
      "Epoch 149/200, Average Loss: 8.4685\n",
      "Epoch 150/200, Average Loss: 4.9860\n",
      "Epoch 151/200, Average Loss: 5.2454\n",
      "Epoch 152/200, Average Loss: 4.9505\n",
      "Epoch 153/200, Average Loss: 4.5319\n",
      "Epoch 154/200, Average Loss: 4.1618\n",
      "Epoch 155/200, Average Loss: 5.1156\n",
      "Epoch 156/200, Average Loss: 5.0507\n",
      "Epoch 157/200, Average Loss: 4.0948\n",
      "Epoch 158/200, Average Loss: 4.6558\n",
      "Epoch 159/200, Average Loss: 4.9061\n",
      "Epoch 160/200, Average Loss: 4.5800\n",
      "Epoch 161/200, Average Loss: 4.2776\n",
      "Epoch 162/200, Average Loss: 4.1143\n",
      "Epoch 163/200, Average Loss: 4.7959\n",
      "Epoch 164/200, Average Loss: 5.2601\n",
      "Epoch 165/200, Average Loss: 4.9947\n",
      "Epoch 166/200, Average Loss: 5.0269\n",
      "Epoch 167/200, Average Loss: 5.6629\n",
      "Epoch 168/200, Average Loss: 4.9973\n",
      "Epoch 169/200, Average Loss: 4.5949\n",
      "Epoch 170/200, Average Loss: 4.3298\n",
      "Epoch 171/200, Average Loss: 3.7199\n",
      "Epoch 172/200, Average Loss: 4.1081\n",
      "Epoch 173/200, Average Loss: 4.1206\n",
      "Epoch 174/200, Average Loss: 3.6382\n",
      "Epoch 175/200, Average Loss: 4.1834\n",
      "Epoch 176/200, Average Loss: 4.2015\n",
      "Epoch 177/200, Average Loss: 4.9093\n",
      "Epoch 178/200, Average Loss: 5.9091\n",
      "Epoch 179/200, Average Loss: 6.2137\n",
      "Epoch 180/200, Average Loss: 5.4129\n",
      "Epoch 181/200, Average Loss: 6.4366\n",
      "Epoch 182/200, Average Loss: 4.7130\n",
      "Epoch 183/200, Average Loss: 7.6768\n",
      "Epoch 184/200, Average Loss: 5.4479\n",
      "Epoch 185/200, Average Loss: 5.6378\n",
      "Epoch 186/200, Average Loss: 4.8994\n",
      "Epoch 187/200, Average Loss: 4.9462\n",
      "Epoch 188/200, Average Loss: 7.0548\n",
      "Epoch 189/200, Average Loss: 4.3857\n",
      "Epoch 190/200, Average Loss: 7.3153\n",
      "Epoch 191/200, Average Loss: 5.6291\n",
      "Epoch 192/200, Average Loss: 4.6919\n",
      "Epoch 193/200, Average Loss: 3.7618\n",
      "Epoch 194/200, Average Loss: 5.0455\n",
      "Epoch 195/200, Average Loss: 6.1678\n",
      "Epoch 196/200, Average Loss: 4.6050\n",
      "Epoch 197/200, Average Loss: 4.5184\n",
      "Epoch 198/200, Average Loss: 5.8717\n",
      "Epoch 199/200, Average Loss: 6.0378\n",
      "Epoch 200/200, Average Loss: 5.3117\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode. This is \n",
    "    # important because certain layers, such \n",
    "    # as dropout or batch normalization, behave\n",
    "    # differently during training than during \n",
    "    # evaluation. model.train() ensures that \n",
    "    # these layers are in training mode.\n",
    "    model.train()\n",
    "    total_loss = 0  # Initialize total loss for the epoch\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Resets the gradients of all model parameters\n",
    "        # to zero before starting the backpropagation\n",
    "        # process for the current batch. This is \n",
    "        # important because gradients are accumulated\n",
    "        # by default in PyTorch, so they need to be\n",
    "        # cleared out before calculating the gradients\n",
    "        # for the current batch.\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        # Compute the loss between the model's \n",
    "        # predictions and the actual target values \n",
    "        # (batch_y).\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        # Compute the gradients of the loss with \n",
    "        # respect to each model parameter using \n",
    "        # backpropagation. These gradients are \n",
    "        # used to update the model parameters \n",
    "        # in the next step.\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss for each batch\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)  # Compute the average loss for the epoch    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Average Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23c8163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.3876\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "# Switch to Evaluation Mode. In this mode, \n",
    "# certain layers like dropout and batch normalization, \n",
    "# which behave differently during training, will \n",
    "# operate in evaluation mode, meaning they won't \n",
    "# apply dropout or update running statistics.\n",
    "# Why Use eval()?: This ensures that the \n",
    "# model's behavior is consistent during \n",
    "# testing and that the evaluation reflects\n",
    "# the true performance on unseen data.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Disabling Gradient Calculation.\n",
    "# The torch.no_grad() context manager \n",
    "# temporarily disables gradient computation. \n",
    "# Since gradients are only necessary during \n",
    "# training (when you need to update the \n",
    "# model's parameters), disabling them \n",
    "# during evaluation saves memory and \n",
    "# computational resources because\n",
    "# pytorch will not track the operations\n",
    "# for that it might need later for gradient\n",
    "# computation.\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(\n",
    "        test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af858ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[439.6256],\n",
       "         [508.6884],\n",
       "         [328.9977],\n",
       "         [494.3349],\n",
       "         [483.5022],\n",
       "         [475.4073],\n",
       "         [438.1123],\n",
       "         [533.4117],\n",
       "         [326.2802],\n",
       "         [371.2693],\n",
       "         [357.7087],\n",
       "         [429.1160]]),\n",
       " tensor([[430.4150],\n",
       "         [500.0000],\n",
       "         [316.7512],\n",
       "         [490.2917],\n",
       "         [487.6102],\n",
       "         [477.7354],\n",
       "         [420.0000],\n",
       "         [535.8058],\n",
       "         [314.3954],\n",
       "         [365.5778],\n",
       "         [350.0000],\n",
       "         [426.8807]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[test_predictions, y_test_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1faa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights and biases for each layer in the model\n",
    "def print_weights_and_biases(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"Values:\\n{param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64e196a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1.weight\n",
      "Values:\n",
      "tensor([[-0.2838, -0.5385,  0.4280],\n",
      "        [ 0.8322, -0.6214,  0.6849],\n",
      "        [-0.4141,  0.9010, -0.7471],\n",
      "        [ 0.9529, -1.0301,  0.7717],\n",
      "        [ 0.7257, -0.2151,  0.8177],\n",
      "        [-0.1737, -0.7952, -0.5588],\n",
      "        [-0.6753,  0.3643, -0.2603],\n",
      "        [-0.8063,  0.5553, -0.8816],\n",
      "        [ 0.8918, -0.6487,  0.8875],\n",
      "        [ 0.4598, -0.5047,  0.0798],\n",
      "        [ 0.4995, -0.2721,  0.5186],\n",
      "        [-0.5367,  0.1293, -0.1387],\n",
      "        [ 0.2091,  0.6143, -0.6836],\n",
      "        [-0.6738,  0.3797,  0.3034],\n",
      "        [-0.0103, -0.3266, -0.2405],\n",
      "        [ 0.0361,  0.0218,  0.3611],\n",
      "        [-0.4846,  0.5205, -0.0505],\n",
      "        [-0.1042,  0.5929, -0.2307],\n",
      "        [ 0.4953,  0.2424, -0.1432],\n",
      "        [ 0.8315, -0.4074,  0.5371],\n",
      "        [-0.6415,  0.3578, -0.2271],\n",
      "        [-0.6294,  0.1296,  0.0137],\n",
      "        [-0.8083, -0.3085, -0.4704],\n",
      "        [ 0.0119, -0.4073,  0.7258],\n",
      "        [-0.3855,  0.0222,  0.2902],\n",
      "        [-0.4356,  0.3317, -0.1935],\n",
      "        [-0.7392,  0.6251, -0.5321],\n",
      "        [ 0.3929,  0.2096,  0.0797],\n",
      "        [-0.2591,  0.0978, -0.0446],\n",
      "        [ 0.1458,  0.4378, -0.5681],\n",
      "        [ 0.6127, -0.0112, -0.2469],\n",
      "        [-0.8359,  0.4683, -0.7923]])\n",
      "\n",
      "Layer: fc1.bias\n",
      "Values:\n",
      "tensor([ 1.5409,  0.1895,  0.4102, -0.0933,  1.1563,  1.0568,  1.5446, -0.7187,\n",
      "         1.2096,  1.6248,  0.7084,  1.0042,  1.4342,  1.3978,  1.4257,  1.5025,\n",
      "         1.4228,  1.3398,  1.2455,  1.0054,  1.4684,  1.3985,  0.8463,  0.8872,\n",
      "        -1.4235,  1.3032, -0.6137,  1.4366, -1.4814,  1.2234,  1.3340, -0.6844])\n",
      "\n",
      "Layer: fc2.weight\n",
      "Values:\n",
      "tensor([[-0.1061, -0.1646, -0.2341,  ..., -0.0274, -0.1177,  0.2855],\n",
      "        [ 0.7522,  0.5251,  0.4380,  ...,  0.5567,  0.8077, -1.1644],\n",
      "        [-0.8077, -0.3382, -0.4515,  ..., -0.3852, -0.7732,  1.2146],\n",
      "        ...,\n",
      "        [ 0.7046,  0.5958,  0.4858,  ...,  0.3798,  0.7907, -1.2293],\n",
      "        [-0.6517, -0.5095, -0.5049,  ..., -0.5380, -0.9082,  1.2498],\n",
      "        [ 0.8503,  0.5074,  0.5690,  ...,  0.6136,  0.6850, -1.0547]])\n",
      "\n",
      "Layer: fc2.bias\n",
      "Values:\n",
      "tensor([-0.1690,  0.6774, -0.9794,  0.7093,  0.9911,  0.8803,  0.9250,  0.9327,\n",
      "         0.9284,  0.9051, -0.7542, -0.1161, -0.9451,  0.6792,  0.8633,  0.9292,\n",
      "         0.7395, -0.2474, -0.2850, -1.0146, -0.5669, -0.6378, -0.4016, -1.0069,\n",
      "         0.7449,  1.0198, -0.9323,  0.9955,  0.8545, -0.9536, -0.0303, -0.5304,\n",
      "        -0.3498,  0.0221, -0.7239,  0.7220,  0.9361,  0.6708,  0.8240,  0.7315,\n",
      "         0.9025,  0.8363, -0.5623, -0.9983, -0.1164,  0.8913,  0.9701, -0.8784,\n",
      "         0.3937, -0.1797, -0.6326,  0.8255, -0.1877, -0.9609,  0.9101,  0.9980,\n",
      "         0.7877,  0.8587, -0.4314,  0.6782,  0.8912,  0.6894, -0.9992,  0.9664])\n",
      "\n",
      "Layer: fc3.weight\n",
      "Values:\n",
      "tensor([[-0.0668,  0.5965, -0.6978,  0.6402,  0.6082,  0.6058,  0.6625,  0.7348,\n",
      "          0.5869,  0.6056, -0.6929, -0.0736, -0.5883,  0.6828,  0.6227,  0.6506,\n",
      "          0.6412, -0.0786, -0.0322, -0.6340, -0.5118, -0.7186, -0.5240, -0.6379,\n",
      "          0.6240,  0.6445, -0.6292,  0.7005,  0.6919, -0.6114,  0.0371, -0.4922,\n",
      "         -0.4570,  0.0063, -0.7225,  0.6444,  0.6851,  0.6450,  0.7235,  0.7033,\n",
      "          0.7074,  0.7110, -0.4527, -0.6801, -0.1518,  0.6983,  0.6248, -0.7096,\n",
      "          0.1486, -0.3968, -0.5473,  0.6217, -0.0197, -0.6827,  0.6186,  0.6417,\n",
      "          0.6486,  0.5465, -0.5465,  0.5841,  0.6130,  0.7316, -0.6746,  0.6481]])\n",
      "\n",
      "Layer: fc3.bias\n",
      "Values:\n",
      "tensor([0.7042])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to print weights and biases\n",
    "print_weights_and_biases(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e39717",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = [[90, 12, 52],\n",
    "          [83, 15, 50],\n",
    "          [100, 2, 90]]\n",
    "\n",
    "myData_transformed = scaler.transform(myData)\n",
    "\n",
    "myData_tensor= torch.tensor(\n",
    "    myData_transformed, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65401cba-08cf-42d4-8f1f-235517e9ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(myData_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c2f5801-777d-49cf-8acb-bbfb47169319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[493.8405],\n",
       "        [470.1354],\n",
       "        [769.3289]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698087e7-dd54-4fc2-bdab-e56d03e7eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[90, 12, 52], [83, 15, 50], [100, 2, 90]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f17016ba-d822-469d-9a3d-7e7fa4afaae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53314162, -1.42914842,  0.42526729],\n",
       "       [ 0.18871365, -0.98567586,  0.22595528],\n",
       "       [ 1.02518157, -2.90739028,  4.21219547]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79616d-7ec1-4fd3-ab96-f673a3cbc9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
