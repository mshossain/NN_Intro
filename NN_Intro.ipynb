{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9345fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74716e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Pecan.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846481e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x14af2c89dd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['Salinity level'], df['Pecan Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09911309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Row ID column\n",
    "data = df.drop(columns=['Row ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4b1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input (features) and output (target)\n",
    "X = data.drop(columns=['Pecan Yield']).values\n",
    "y = data['Pecan Yield'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f591d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch scikit-learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a46a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#  Specifies the proportion of the dataset that \n",
    "# should be allocated to the test set. \n",
    "# Here, 0.2 means 20% of the data will be used\n",
    "# for testing, and the remaining 80% will be \n",
    "# used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49de71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# StandardScaler: This is a class from \n",
    "# Scikit-learn used to standardize \n",
    "# features by removing the mean and \n",
    "# scaling to unit variance.\n",
    "# Standardization: This process involves\n",
    "# rescaling the features so that they have \n",
    "# a mean of 0 and a standard deviation of 1.\n",
    "# This is important for many machine learning\n",
    "# algorithms that perform better when \n",
    "# features are on a similar scale.\n",
    "\n",
    "# This process is crucial for ensuring \n",
    "# that the model performs consistently,\n",
    "# as many machine learning algorithms \n",
    "# assume or perform better when the \n",
    "# input features are on a similar scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0743cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(\n",
    "    X_train, dtype=torch.float32)\n",
    "\n",
    "# torch.tensor(X_train): Converts \n",
    "# the X_train data (which is likely \n",
    "# a NumPy array or a Pandas \n",
    "# DataFrame) into a PyTorch tensor.\n",
    "# dtype=torch.float32: Specifies that \n",
    "# the data type of the tensor should \n",
    "# be float32, which is a common \n",
    "# choice for numerical data in \n",
    "# deep learning models.\n",
    "# Result: X_train_tensor is now a \n",
    "# PyTorch tensor containing the \n",
    "# standardized training input data, \n",
    "# ready for use in a neural network.\n",
    "\n",
    "y_train_tensor = torch.tensor(\n",
    "    y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# torch.tensor(y_train): Converts the \n",
    "# y_train data (the target variable \n",
    "# for training) into a PyTorch tensor.\n",
    "# .view(-1, 1): Reshapes the tensor \n",
    "# to have a shape of (-1, 1). \n",
    "# Here, -1 is a placeholder that tells \n",
    "# PyTorch to infer the correct size for \n",
    "# this dimension based on the other \n",
    "# dimensions. This ensures \n",
    "# that y_train_tensor has two dimensions, \n",
    "# with the second dimension being 1 \n",
    "# (i.e., a column vector).\n",
    "# Result: y_train_tensor is now a 2D \n",
    "# tensor with shape (n_samples, 1), \n",
    "# where n_samples is the number of \n",
    "# training samples.\n",
    "\n",
    "X_test_tensor = torch.tensor(\n",
    "    X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(\n",
    "    y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df318eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(\n",
    "    X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fac1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24214ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "#  defines a new class NeuralNetwork \n",
    "# that inherits from nn.Module. \n",
    "# By inheriting from nn.Module, \n",
    "# the NeuralNetwork class gains access \n",
    "# to all the functionalities provided \n",
    "# by PyTorch for building and managing neural networks.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self): # Constructor\n",
    "        # Start by calling the constructor of \n",
    "        # the parent class (nn.Module), ensuring \n",
    "        # that the class is properly initialized.\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # nn.Linear creates a fully connected (linear) layer.\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    # The forward method defines the forward pass of \n",
    "    # the network. It specifies how the input tensor\n",
    "    # x should flow through the layers of the network.\n",
    "    def forward(self, x):\n",
    "        #x = torch.relu(self.fc1(x)) \n",
    "        #x = torch.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        \n",
    "        x = self.fc1(x)  # No activation after the first layer\n",
    "        x = self.fc2(x)  # No activation after the second layer\n",
    "        x = self.fc3(x)  # Output layer        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26d9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.L1Loss() # for mean absoluste loss\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=0.01)\n",
    "\n",
    "# optim.Adam: This creates an optimizer using the \n",
    "# Adam algorithm, which is a popular optimization\n",
    "# algorithm that combines the advantages of two \n",
    "# other algorithms: AdaGrad and RMSProp. It adjusts\n",
    "# the learning rate for each parameter individually\n",
    "# based on estimates of lower-order moments.\n",
    "# model.parameters(): This passes the model's\n",
    "# parameters (weights and biases) to the optimizer.\n",
    "# The optimizer will update these parameters during\n",
    "# training based on the computed gradients.\n",
    "# lr=0.01: This sets the learning rate, which \n",
    "# controls how much to adjust the model parameters\n",
    "# with respect to the gradient. A learning rate of\n",
    "# 0.01 is relatively standard, but it may need \n",
    "# tuning depending on the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735c4f5-d7b3-42cb-891e-aa7f698ad04c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0f67c-6678-4cdc-a700-0747c7b7f91e",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b54385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Average Loss: 438.8109\n",
      "Epoch 2/200, Average Loss: 436.4712\n",
      "Epoch 3/200, Average Loss: 432.5754\n",
      "Epoch 4/200, Average Loss: 430.0527\n",
      "Epoch 5/200, Average Loss: 422.9325\n",
      "Epoch 6/200, Average Loss: 412.0533\n",
      "Epoch 7/200, Average Loss: 402.6561\n",
      "Epoch 8/200, Average Loss: 385.1822\n",
      "Epoch 9/200, Average Loss: 363.3822\n",
      "Epoch 10/200, Average Loss: 335.3332\n",
      "Epoch 11/200, Average Loss: 297.6929\n",
      "Epoch 12/200, Average Loss: 251.8781\n",
      "Epoch 13/200, Average Loss: 196.0149\n",
      "Epoch 14/200, Average Loss: 126.5973\n",
      "Epoch 15/200, Average Loss: 51.7624\n",
      "Epoch 16/200, Average Loss: 47.3913\n",
      "Epoch 17/200, Average Loss: 83.8905\n",
      "Epoch 18/200, Average Loss: 68.5392\n",
      "Epoch 19/200, Average Loss: 24.8489\n",
      "Epoch 20/200, Average Loss: 24.0249\n",
      "Epoch 21/200, Average Loss: 37.2312\n",
      "Epoch 22/200, Average Loss: 20.8305\n",
      "Epoch 23/200, Average Loss: 15.8365\n",
      "Epoch 24/200, Average Loss: 17.8872\n",
      "Epoch 25/200, Average Loss: 10.9573\n",
      "Epoch 26/200, Average Loss: 15.1426\n",
      "Epoch 27/200, Average Loss: 16.9389\n",
      "Epoch 28/200, Average Loss: 11.9505\n",
      "Epoch 29/200, Average Loss: 18.9451\n",
      "Epoch 30/200, Average Loss: 13.3334\n",
      "Epoch 31/200, Average Loss: 11.2614\n",
      "Epoch 32/200, Average Loss: 12.1081\n",
      "Epoch 33/200, Average Loss: 8.6294\n",
      "Epoch 34/200, Average Loss: 8.0660\n",
      "Epoch 35/200, Average Loss: 9.0711\n",
      "Epoch 36/200, Average Loss: 8.0544\n",
      "Epoch 37/200, Average Loss: 6.7591\n",
      "Epoch 38/200, Average Loss: 5.9119\n",
      "Epoch 39/200, Average Loss: 7.7931\n",
      "Epoch 40/200, Average Loss: 6.9517\n",
      "Epoch 41/200, Average Loss: 6.5559\n",
      "Epoch 42/200, Average Loss: 5.9891\n",
      "Epoch 43/200, Average Loss: 6.4782\n",
      "Epoch 44/200, Average Loss: 5.7320\n",
      "Epoch 45/200, Average Loss: 5.9237\n",
      "Epoch 46/200, Average Loss: 5.8118\n",
      "Epoch 47/200, Average Loss: 5.6857\n",
      "Epoch 48/200, Average Loss: 6.1304\n",
      "Epoch 49/200, Average Loss: 5.5722\n",
      "Epoch 50/200, Average Loss: 5.8637\n",
      "Epoch 51/200, Average Loss: 5.7876\n",
      "Epoch 52/200, Average Loss: 6.1401\n",
      "Epoch 53/200, Average Loss: 6.1752\n",
      "Epoch 54/200, Average Loss: 5.9087\n",
      "Epoch 55/200, Average Loss: 5.7863\n",
      "Epoch 56/200, Average Loss: 6.5916\n",
      "Epoch 57/200, Average Loss: 6.7854\n",
      "Epoch 58/200, Average Loss: 6.7533\n",
      "Epoch 59/200, Average Loss: 6.5290\n",
      "Epoch 60/200, Average Loss: 8.0851\n",
      "Epoch 61/200, Average Loss: 8.9817\n",
      "Epoch 62/200, Average Loss: 6.7201\n",
      "Epoch 63/200, Average Loss: 6.9540\n",
      "Epoch 64/200, Average Loss: 6.8673\n",
      "Epoch 65/200, Average Loss: 6.8231\n",
      "Epoch 66/200, Average Loss: 6.6494\n",
      "Epoch 67/200, Average Loss: 6.8687\n",
      "Epoch 68/200, Average Loss: 6.1796\n",
      "Epoch 69/200, Average Loss: 5.8292\n",
      "Epoch 70/200, Average Loss: 5.7820\n",
      "Epoch 71/200, Average Loss: 6.4471\n",
      "Epoch 72/200, Average Loss: 6.0389\n",
      "Epoch 73/200, Average Loss: 6.1339\n",
      "Epoch 74/200, Average Loss: 5.8340\n",
      "Epoch 75/200, Average Loss: 5.4631\n",
      "Epoch 76/200, Average Loss: 5.9323\n",
      "Epoch 77/200, Average Loss: 6.2362\n",
      "Epoch 78/200, Average Loss: 6.2956\n",
      "Epoch 79/200, Average Loss: 6.5291\n",
      "Epoch 80/200, Average Loss: 6.2058\n",
      "Epoch 81/200, Average Loss: 6.3326\n",
      "Epoch 82/200, Average Loss: 8.5748\n",
      "Epoch 83/200, Average Loss: 7.1009\n",
      "Epoch 84/200, Average Loss: 8.1719\n",
      "Epoch 85/200, Average Loss: 7.8606\n",
      "Epoch 86/200, Average Loss: 7.8648\n",
      "Epoch 87/200, Average Loss: 8.9086\n",
      "Epoch 88/200, Average Loss: 7.8968\n",
      "Epoch 89/200, Average Loss: 10.2182\n",
      "Epoch 90/200, Average Loss: 8.6046\n",
      "Epoch 91/200, Average Loss: 10.7381\n",
      "Epoch 92/200, Average Loss: 9.2195\n",
      "Epoch 93/200, Average Loss: 9.8186\n",
      "Epoch 94/200, Average Loss: 7.0221\n",
      "Epoch 95/200, Average Loss: 7.7599\n",
      "Epoch 96/200, Average Loss: 7.0900\n",
      "Epoch 97/200, Average Loss: 6.9597\n",
      "Epoch 98/200, Average Loss: 5.9602\n",
      "Epoch 99/200, Average Loss: 7.0206\n",
      "Epoch 100/200, Average Loss: 6.7833\n",
      "Epoch 101/200, Average Loss: 6.5533\n",
      "Epoch 102/200, Average Loss: 6.7843\n",
      "Epoch 103/200, Average Loss: 7.8896\n",
      "Epoch 104/200, Average Loss: 6.9000\n",
      "Epoch 105/200, Average Loss: 6.8270\n",
      "Epoch 106/200, Average Loss: 6.8833\n",
      "Epoch 107/200, Average Loss: 6.9383\n",
      "Epoch 108/200, Average Loss: 8.2355\n",
      "Epoch 109/200, Average Loss: 7.2105\n",
      "Epoch 110/200, Average Loss: 6.6474\n",
      "Epoch 111/200, Average Loss: 5.9139\n",
      "Epoch 112/200, Average Loss: 6.2876\n",
      "Epoch 113/200, Average Loss: 6.7790\n",
      "Epoch 114/200, Average Loss: 6.8621\n",
      "Epoch 115/200, Average Loss: 6.4593\n",
      "Epoch 116/200, Average Loss: 7.2173\n",
      "Epoch 117/200, Average Loss: 5.8102\n",
      "Epoch 118/200, Average Loss: 6.3065\n",
      "Epoch 119/200, Average Loss: 5.6628\n",
      "Epoch 120/200, Average Loss: 5.9296\n",
      "Epoch 121/200, Average Loss: 5.8106\n",
      "Epoch 122/200, Average Loss: 6.0830\n",
      "Epoch 123/200, Average Loss: 5.6010\n",
      "Epoch 124/200, Average Loss: 7.3141\n",
      "Epoch 125/200, Average Loss: 5.9980\n",
      "Epoch 126/200, Average Loss: 5.5817\n",
      "Epoch 127/200, Average Loss: 5.9444\n",
      "Epoch 128/200, Average Loss: 5.7696\n",
      "Epoch 129/200, Average Loss: 6.0568\n",
      "Epoch 130/200, Average Loss: 6.7042\n",
      "Epoch 131/200, Average Loss: 6.7155\n",
      "Epoch 132/200, Average Loss: 6.2449\n",
      "Epoch 133/200, Average Loss: 5.6797\n",
      "Epoch 134/200, Average Loss: 6.1684\n",
      "Epoch 135/200, Average Loss: 5.4143\n",
      "Epoch 136/200, Average Loss: 5.9451\n",
      "Epoch 137/200, Average Loss: 6.6527\n",
      "Epoch 138/200, Average Loss: 7.4130\n",
      "Epoch 139/200, Average Loss: 6.3716\n",
      "Epoch 140/200, Average Loss: 6.9714\n",
      "Epoch 141/200, Average Loss: 8.0027\n",
      "Epoch 142/200, Average Loss: 7.0624\n",
      "Epoch 143/200, Average Loss: 7.7231\n",
      "Epoch 144/200, Average Loss: 7.3114\n",
      "Epoch 145/200, Average Loss: 7.5457\n",
      "Epoch 146/200, Average Loss: 6.0742\n",
      "Epoch 147/200, Average Loss: 7.1022\n",
      "Epoch 148/200, Average Loss: 8.6558\n",
      "Epoch 149/200, Average Loss: 5.7651\n",
      "Epoch 150/200, Average Loss: 7.3815\n",
      "Epoch 151/200, Average Loss: 6.4333\n",
      "Epoch 152/200, Average Loss: 5.7191\n",
      "Epoch 153/200, Average Loss: 6.6910\n",
      "Epoch 154/200, Average Loss: 7.0989\n",
      "Epoch 155/200, Average Loss: 6.0786\n",
      "Epoch 156/200, Average Loss: 5.7550\n",
      "Epoch 157/200, Average Loss: 6.3844\n",
      "Epoch 158/200, Average Loss: 7.0241\n",
      "Epoch 159/200, Average Loss: 6.8119\n",
      "Epoch 160/200, Average Loss: 6.7592\n",
      "Epoch 161/200, Average Loss: 6.1645\n",
      "Epoch 162/200, Average Loss: 5.7976\n",
      "Epoch 163/200, Average Loss: 5.4651\n",
      "Epoch 164/200, Average Loss: 6.3339\n",
      "Epoch 165/200, Average Loss: 7.4696\n",
      "Epoch 166/200, Average Loss: 6.4156\n",
      "Epoch 167/200, Average Loss: 8.7625\n",
      "Epoch 168/200, Average Loss: 6.5203\n",
      "Epoch 169/200, Average Loss: 7.0895\n",
      "Epoch 170/200, Average Loss: 5.9390\n",
      "Epoch 171/200, Average Loss: 6.1723\n",
      "Epoch 172/200, Average Loss: 5.9782\n",
      "Epoch 173/200, Average Loss: 5.9035\n",
      "Epoch 174/200, Average Loss: 6.3483\n",
      "Epoch 175/200, Average Loss: 5.6415\n",
      "Epoch 176/200, Average Loss: 5.9031\n",
      "Epoch 177/200, Average Loss: 6.0337\n",
      "Epoch 178/200, Average Loss: 5.7973\n",
      "Epoch 179/200, Average Loss: 6.0769\n",
      "Epoch 180/200, Average Loss: 5.9047\n",
      "Epoch 181/200, Average Loss: 6.1174\n",
      "Epoch 182/200, Average Loss: 6.4154\n",
      "Epoch 183/200, Average Loss: 7.0855\n",
      "Epoch 184/200, Average Loss: 5.8786\n",
      "Epoch 185/200, Average Loss: 5.8131\n",
      "Epoch 186/200, Average Loss: 6.0681\n",
      "Epoch 187/200, Average Loss: 6.5762\n",
      "Epoch 188/200, Average Loss: 5.5539\n",
      "Epoch 189/200, Average Loss: 5.8394\n",
      "Epoch 190/200, Average Loss: 6.7001\n",
      "Epoch 191/200, Average Loss: 8.1827\n",
      "Epoch 192/200, Average Loss: 5.4885\n",
      "Epoch 193/200, Average Loss: 8.9397\n",
      "Epoch 194/200, Average Loss: 8.7347\n",
      "Epoch 195/200, Average Loss: 6.7186\n",
      "Epoch 196/200, Average Loss: 7.5119\n",
      "Epoch 197/200, Average Loss: 6.9747\n",
      "Epoch 198/200, Average Loss: 6.6012\n",
      "Epoch 199/200, Average Loss: 6.4704\n",
      "Epoch 200/200, Average Loss: 7.6627\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode. This is \n",
    "    # important because certain layers, such \n",
    "    # as dropout or batch normalization, behave\n",
    "    # differently during training than during \n",
    "    # evaluation. model.train() ensures that \n",
    "    # these layers are in training mode.\n",
    "    model.train()\n",
    "    total_loss = 0  # Initialize total loss for the epoch\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Resets the gradients of all model parameters\n",
    "        # to zero before starting the backpropagation\n",
    "        # process for the current batch. This is \n",
    "        # important because gradients are accumulated\n",
    "        # by default in PyTorch, so they need to be\n",
    "        # cleared out before calculating the gradients\n",
    "        # for the current batch.\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        # Compute the loss between the model's \n",
    "        # predictions and the actual target values \n",
    "        # (batch_y).\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        # Compute the gradients of the loss with \n",
    "        # respect to each model parameter using \n",
    "        # backpropagation. These gradients are \n",
    "        # used to update the model parameters \n",
    "        # in the next step.\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss for each batch\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)  # Compute the average loss for the epoch    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Average Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23c8163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.3963\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "# Switch to Evaluation Mode. In this mode, \n",
    "# certain layers like dropout and batch normalization, \n",
    "# which behave differently during training, will \n",
    "# operate in evaluation mode, meaning they won't \n",
    "# apply dropout or update running statistics.\n",
    "# Why Use eval()?: This ensures that the \n",
    "# model's behavior is consistent during \n",
    "# testing and that the evaluation reflects\n",
    "# the true performance on unseen data.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Disabling Gradient Calculation.\n",
    "# The torch.no_grad() context manager \n",
    "# temporarily disables gradient computation. \n",
    "# Since gradients are only necessary during \n",
    "# training (when you need to update the \n",
    "# model's parameters), disabling them \n",
    "# during evaluation saves memory and \n",
    "# computational resources because\n",
    "# pytorch will not track the operations\n",
    "# for that it might need later for gradient\n",
    "# computation.\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(\n",
    "        test_predictions, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af858ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[433.4372],\n",
       "         [500.3364],\n",
       "         [330.0754],\n",
       "         [492.0927],\n",
       "         [483.1656],\n",
       "         [475.5349],\n",
       "         [428.6643],\n",
       "         [521.8048],\n",
       "         [327.0096],\n",
       "         [371.8168],\n",
       "         [355.8466],\n",
       "         [410.6195]]),\n",
       " tensor([[430.4150],\n",
       "         [500.0000],\n",
       "         [316.7512],\n",
       "         [490.2917],\n",
       "         [487.6102],\n",
       "         [477.7354],\n",
       "         [420.0000],\n",
       "         [535.8058],\n",
       "         [314.3954],\n",
       "         [365.5778],\n",
       "         [350.0000],\n",
       "         [426.8807]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[test_predictions, y_test_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1faa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights and biases for each layer in the model\n",
    "def print_weights_and_biases(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"Values:\\n{param.data}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64e196a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1.weight\n",
      "Values:\n",
      "tensor([[-0.2005,  0.5418,  0.0232],\n",
      "        [ 0.1139,  0.1023,  0.3740],\n",
      "        [-0.5391, -0.0471, -0.0766],\n",
      "        [ 0.4257,  0.0399, -0.0266],\n",
      "        [-0.1979,  0.3697, -0.0980],\n",
      "        [-0.2770, -0.2382,  0.4937],\n",
      "        [ 0.3037,  0.5256,  0.3834],\n",
      "        [-0.1582,  0.3884, -0.3929],\n",
      "        [-0.1862, -0.2714, -0.3555],\n",
      "        [-0.1541,  0.0327,  0.4901],\n",
      "        [-0.4332,  0.0786, -0.0794],\n",
      "        [ 0.1905,  0.2579, -0.2056],\n",
      "        [-0.2257, -0.5405, -0.1266],\n",
      "        [-0.3973, -0.0639, -0.3027],\n",
      "        [ 0.2357,  0.5266, -0.2322],\n",
      "        [-0.6047, -0.4560, -0.0292],\n",
      "        [ 0.4225, -0.1357,  0.0501],\n",
      "        [-0.4060, -0.3712,  0.3301],\n",
      "        [ 0.4473, -0.3259, -0.1911],\n",
      "        [ 0.4498, -0.2634,  0.2146],\n",
      "        [-0.5136,  0.4102, -0.0367],\n",
      "        [ 0.2394,  0.3013, -0.2215],\n",
      "        [ 0.3505, -0.4492, -0.1237],\n",
      "        [ 0.4592,  0.0072,  0.3793],\n",
      "        [-0.3043, -0.3420,  0.2944],\n",
      "        [-0.6196,  0.4644,  0.2684],\n",
      "        [-0.1931, -0.1374, -0.4793],\n",
      "        [-0.2483,  0.4620,  0.2824],\n",
      "        [ 0.4018,  0.1182,  0.1163],\n",
      "        [ 0.1101, -0.1065, -0.4459],\n",
      "        [-0.5678, -0.0873, -0.3530],\n",
      "        [-0.2158, -0.4516,  0.5318]])\n",
      "\n",
      "Layer: fc1.bias\n",
      "Values:\n",
      "tensor([-1.0479, -0.9048, -0.5861, -0.9089,  0.8578,  0.8432, -0.6414,  0.9159,\n",
      "        -1.0230,  0.9863,  0.9882,  1.0470, -0.8179,  0.4669,  0.6184, -0.6269,\n",
      "         0.9821,  0.7945,  0.8569,  0.8749, -0.7488, -0.8144,  0.5056, -0.7393,\n",
      "         0.6565, -0.8848,  0.4818,  0.9145,  0.7801, -0.7362, -0.4858,  0.8262])\n",
      "\n",
      "Layer: fc2.weight\n",
      "Values:\n",
      "tensor([[-0.5517, -0.5579, -0.4405,  ..., -0.5857, -0.4401,  0.3418],\n",
      "        [ 0.5917,  0.4097,  0.3183,  ...,  0.4300,  0.2279, -0.4480],\n",
      "        [-0.6526, -0.4899, -0.6261,  ..., -0.3698, -0.3696,  0.4342],\n",
      "        ...,\n",
      "        [-0.6404, -0.3756, -0.5173,  ..., -0.5884, -0.2424,  0.5310],\n",
      "        [-0.5470, -0.6843, -0.4552,  ..., -0.6114, -0.3290,  0.4744],\n",
      "        [ 0.5400,  0.3906,  0.3131,  ...,  0.3431,  0.3969, -0.3281]])\n",
      "\n",
      "Layer: fc2.bias\n",
      "Values:\n",
      "tensor([ 0.6921, -0.6182,  0.7234,  0.4878, -0.6651,  0.5649, -0.6478,  0.5398,\n",
      "         0.4836, -0.3415,  0.7319, -0.6812,  0.6449,  0.5451,  0.4333, -0.3279,\n",
      "        -0.5644, -0.7330, -0.5408, -0.5716,  0.6894,  0.7483,  0.6449, -0.6201,\n",
      "        -0.4502,  0.7045, -0.6697, -0.6537, -0.7070,  0.5533, -0.6669,  0.5870,\n",
      "        -0.4830, -0.6460,  0.7091, -0.6382,  0.6935, -0.4548, -0.4724, -0.4733,\n",
      "         0.7472, -0.5971,  0.7073, -0.7133,  0.4037,  0.4497, -0.5992,  0.5444,\n",
      "        -0.6011, -0.6870,  0.6269, -0.6511, -0.7013,  0.5650,  0.4233, -0.7273,\n",
      "         0.4497, -0.6133,  0.5687, -0.6417,  0.6623,  0.6303,  0.4425, -0.4496])\n",
      "\n",
      "Layer: fc3.weight\n",
      "Values:\n",
      "tensor([[ 0.6113, -0.5833,  0.5074,  0.5740, -0.6100,  0.5234, -0.5943,  0.5798,\n",
      "          0.4823, -0.3679,  0.5457, -0.5301,  0.5055,  0.4101,  0.4851, -0.3972,\n",
      "         -0.4197, -0.6357, -0.5667, -0.5221,  0.5522,  0.5253,  0.5490, -0.4641,\n",
      "         -0.4894,  0.5230, -0.6398, -0.5520, -0.5067,  0.5439, -0.5370,  0.5308,\n",
      "         -0.5797, -0.5397,  0.5236, -0.5067,  0.5766, -0.5624, -0.3382, -0.4458,\n",
      "          0.6178, -0.5473,  0.5208, -0.5726,  0.4912,  0.5064, -0.6080,  0.4937,\n",
      "         -0.5089, -0.4904,  0.5551, -0.5483, -0.5272,  0.5530,  0.5446, -0.5426,\n",
      "          0.5980, -0.4850,  0.4988, -0.5742,  0.6333,  0.6113,  0.5074, -0.4666]])\n",
      "\n",
      "Layer: fc3.bias\n",
      "Values:\n",
      "tensor([0.5628])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function to print weights and biases\n",
    "print_weights_and_biases(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0e39717",
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = [[90, 12, 52],\n",
    "          [83, 15, 50],\n",
    "          [100, 2, 90]]\n",
    "\n",
    "myData_transformed = scaler.transform(myData)\n",
    "\n",
    "myData_tensor= torch.tensor(\n",
    "    myData_transformed, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65401cba-08cf-42d4-8f1f-235517e9ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(myData_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c2f5801-777d-49cf-8acb-bbfb47169319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[486.3408],\n",
       "        [464.7892],\n",
       "        [610.1937]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "698087e7-dd54-4fc2-bdab-e56d03e7eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[90, 12, 52], [83, 15, 50], [100, 2, 90]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f17016ba-d822-469d-9a3d-7e7fa4afaae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53314162, -1.42914842,  0.42526729],\n",
       "       [ 0.18871365, -0.98567586,  0.22595528],\n",
       "       [ 1.02518157, -2.90739028,  4.21219547]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79616d-7ec1-4fd3-ab96-f673a3cbc9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
